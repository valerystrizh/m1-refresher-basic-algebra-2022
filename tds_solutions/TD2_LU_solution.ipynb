{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the last lecture that we were looking at the Gaussian-elimination steps to convert a matrix $A$ into **upper-triangular form** via a sequence of **row operations**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "1 & 1 & -1  \\\\\n",
    "3 & 11 & 6 \n",
    "\\end{pmatrix}}_A\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 2 & 3 \n",
    "\\end{pmatrix}}_{E_1 A}\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 0 & \\boxed{1} \n",
    "\\end{pmatrix}}_{E_2 E_1 A}\n",
    "$$\n",
    "\n",
    "Since row operations correspond to **multiplying by matrices on the *left***, constructed the \"elimination matrices\" $E_1$ and $E_2$ corresponding to each of these elimination steps (under the first and second pivots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we arrived at the formula:\n",
    "$$\n",
    "\\underbrace{E_2 E_1}_E A = U\n",
    "$$\n",
    "Notice that we multiplied $A$ by the elimination matrices from *right to left* in the order of the steps: it is $E_2 E_1 A$, *not* $E_1 E_2 A$.  Because matrix multiplication is generally [not commutative](https://en.wikipedia.org/wiki/Commutative_property), $E_2 E_1$ and $E_1 E_2$ give *different* matrices.\n",
    "\n",
    "Notice, furthermore, that the matrices $E_1$ and $E_2$ are both **lower-triangular matrices**.  This is a consequence of the structure of Gaussian elimination (assuming no row re-ordering): we always add the pivot row to rows *below* it, never *above* it.\n",
    "\n",
    "The *product* of lower-triangular matrices is always lower-triangular too.  In consequence, the product $E = E_2 E_1$ is lower-triangular, and Gaussian elimination can be viewed as yielding\n",
    "\n",
    "$$EA=U$$\n",
    "\n",
    "where $E$ is lower triangular and $U$ is upper triangular!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse elimination: LU factors\n",
    "\n",
    "However, in practice, it turns out to be more useful to write this as\n",
    "\n",
    "$$A= E^{-1} U = E_1^{-1}E_2^{-1}U$$\n",
    "\n",
    "where $E^{-1}$ is the [inverse of the matrix](http://mathworld.wolfram.com/MatrixInverse.html) $E$.  It is the matrix that **reverses the steps** of Gaussian elimination, taking us back from $U$ to $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result, therefore, is that Gaussian elimination (without row swaps) can be viewed as a *factorization* of the original matrix $A$\n",
    "$$\n",
    "A = LU\n",
    "$$\n",
    "into a **product of lower- and upper-triangular matrices**.  (Furthermore, although we didn't comment on this above, $L$ is always 1 along its diagonal.)  This factorization is called the [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition) of $A$.  When a computer performs Gaussian elimination, what it computes are the $L$ and $U$ factors.\n",
    "\n",
    "What this accomplishes is to break a complicated matrix $A$ into **much simpler pieces** $L$ and $U$.  It may not seem at first that $L$ and $U$ are *that* much simpler than $A$, but they are: lots of operations that are very difficult with $A$, like solving equations or computing the determinant, become *easy* once you known $L$ and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting $U$ was \"easy\", it is just Gaussian elimination. But to get $L$, we first write out the individual elimination steps as matrices, then invert them to move them to the other side, then multiply them together to get $L$.\n",
    "\n",
    "However, we can just \"read off\" $L$ much more simply directly from the pivot-row \"multipliers\" that we use during elimination steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We'll do Gaussian elimination on the following 3×3 invertible (non-singular) matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 2 & 5 & 1 \\\\ -3 & 1 & -1 \\end{pmatrix} \\stackrel{r_2 - \\color{red}{2}r_1}{\\stackrel{r_3 + \\color{red}{3}r_1}{\\longrightarrow}}\n",
    "    \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 0 & \\color{blue}{1} & 1 \\\\  0 & 7 & -1 \\end{pmatrix} \\stackrel{r_3 - \\color{red}{7}r_2}{\\longrightarrow}\n",
    "    \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 0 & \\color{blue}{1} & 1 \\\\  0 & 0 & \\color{blue}{-8} \\end{pmatrix} = U\n",
    "$$\n",
    "\n",
    "(Here, \"$r_2 - 2r_1$\" denotes the elimination step \"row 2 - 2(row 1)\" etcetera.)\n",
    "\n",
    "To get $L$, we just need to write down the multipliers as we go along, with **opposite signs**, putting each multiplier in the **same column and row** as the corresponding elimination step:\n",
    "\n",
    "$$\n",
    "L = \\begin{pmatrix} 1 & & \\\\ \\color{red}{+2} & 1 & \\\\ \\color{red}{-3} & \\color{red}{+7} & 1 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Using LU factorizations\n",
    "\n",
    "Lots of things that you might want to do with a matrix $A$ become easier once you have the $A=LU$ factorization.  Most importantly, it becomes much easier to solve systems of equations.\n",
    "\n",
    "(for an $n \\times n$ matrix $A$, it takes around $n^3$ operations to perform Gaussian elimination to get $U$ and $L$, but subsequently solving for $x$ by takes only around $n^2$ operations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ax=b\n",
    "1. Factor $A = LU$ by Gaussian elimination (not including row swaps, discussed below!), giving $Ax = b \\implies LUx = L(Ux) = b$\n",
    "2. Let $c = Ux$.  Solve $Lc = b$ for $c$ by forward-substitution.  (*Note:* this is especially easy because $L$ has only 1's on the diagonal, meaning that there are no divisions.)\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution.\n",
    "\n",
    "The key point to realize is that solving $Lc = b$ for $c$ involves *exactly the same elimination steps* as if you had augmented the matrix with $b$ during Gaussian elimination.   The bookkeeping is more tedious for a human, but computers are good at bookkeeping, and there turn out to be several practical advantages for computer software to separate solving for $LU$ and solving for $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's do another example, this one small enough to do by hand, using our 3×3 example from earlier.  Let's solve:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 0 \\\\ 2 & 5 & 1 \\\\ -3 & 1 & -1 \\end{pmatrix}}_{A = LU} \\underbrace{\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}}_x = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 15 \\\\ -4 \\end{pmatrix}}_b\n",
    "$$\n",
    "\n",
    "First we solve $Lc = b$ by forward substitution:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 &  &  \\\\ 2 & 1 &  \\\\ -3 & 7 & 1 \\end{pmatrix}}_{L} \\underbrace{\\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix}}_c = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 15 \\\\ -4 \\end{pmatrix}}_b\n",
    "$$\n",
    "\n",
    "This immediately gives $c_1 = 5$ from the first row, $2c_1 + c_2 = 10 + c_2 = 15 \\implies c_2 = 5$ from the second row, and $-3c_1 + 7c_2 + c_3 = -15 + 35 + c_3 = -4 \\implies c_3 = -24$  from the third row.  If you look carefully, and remember the Gaussian-elimination steps that we did on $A$, you'll see that these are in fact exactly the same elimination steps applied to $c$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it matches our hand calculation!  Now we solve\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 0 \\\\  & 1 & 1 \\\\  &  & -8 \\end{pmatrix}}_{U} \\underbrace{\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}}_x = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 5 \\\\ -24 \\end{pmatrix}}_c\n",
    "$$\n",
    "\n",
    "by backsubstitution. The third row gives $-8 x_3 = -24 \\implies x_3 = 3$.  The second row gives $x_2 + x_3 = x_2 + 3 = 5 \\implies x_2 = 2$, and the first row gives $x_1 + 2x_2 = x_1 + 4 = 5 \\implies x_1 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row swaps and PA = LU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have mostly ignored the possibility of row swaps.  Row swaps may be *required* if you encounter a zero pivot (assuming there is a nonzero value below it in the same column), but this is often unlikely to occur in practice (especially for random matrices!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LU factorization with row swaps corresponds to a factorization\n",
    "\n",
    "$$\n",
    "PA = LU\n",
    "$$\n",
    "\n",
    "Now, to solve $Ax = b$, a more complete process is:\n",
    "\n",
    "1. Factor $PA = LU$\n",
    "2. Multiply $P$ by both sides to give $PAx = LUx = Pb$\n",
    "3. Let $c=Ux$ and solve $Lc = Pb$ for $c$ by forward-substitution\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular matrices\n",
    "\n",
    "If we encounter a zero pivot (or even just a small pivot, on a computer) during Gaussian elimination, we normally swap rows to bring a nonzero pivot up from a subsequent row.  However, what if there are *no* nonzero values below the pivot in that column?  This is called a [singular matrix](https://en.wikipedia.org/wiki/Invertible_matrix): we can still proceed with Gaussian elimination, but **we can't get rid of the zero pivot**.\n",
    "\n",
    "If you have $Ax=b$ where $A$ is singular, then there will typically (for most right-hand sides $b$) be **no solutions**, but there will occasionally (for very special $b$) be **infinitely many solutions**.  (For $2 \\times 2$ matrices, solving $Ax=b$ corresponds to finding the intersection of two lines, and a singular case corresponds to two parallel lines — either there are no intersections, or they intersect everywhere.)\n",
    "\n",
    "For example, consider the following $4 \\times 4$ matrix $A=LU$:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -1 & 1 &  8 \\\\\n",
    "             6 &  1 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  1 &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "The third pivot in $U$ is zero!   Now, suppose we want to solve $Ax=b$.  We first solve $Lc=b$ to apply the elimination steps to $b$.  This is no problem since $L$ has 1's along the diagonal. Suppose we get $c = (c_1, c_2, c_3, c_4)$.  Then we proceed by backsubstitution to solve $Ux = c$, starting with the last row of $U$:\n",
    "\n",
    "$$\n",
    "1 \\times x_4 = c_4 \\implies x_4 = c_4 \\\\\n",
    "\\color{red}{0 \\times x_3} - 2 \\times x_4 = c_3 \\implies \\mbox{no solution unless } -2 x_4 = -2 c_4 = c_3\n",
    "$$\n",
    "For very special right-hand sides, where $c_3 = 2c_4$, we can plug in *any* $x_3$ and get a solution (infinitely many solutions).  Otherwise, we get *no* solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that singular cases are not very interesting.  In reality, **exactly singular square matrices never occur by accident**.  There is always some *deep structure of the underlying problem* that causes the singularity, and understanding this structure is *always* interesting.\n",
    "\n",
    "On the other hand, **nearly singular** matrices (where the pivots are nonzero but very small) *can* occur by accident, and dealing with them is often a delicate problem because they are very sensitive to roundoff errors.  (We call these matrices [ill-conditioned](https://en.wikipedia.org/wiki/Condition_number).) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrices are **more singular than others**.  For example, they can have **two zero pivots**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  8 \\\\\n",
    "             6 &  3 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **three**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  2 \\\\\n",
    "             6 &  3 & 4 & -2 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **four**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             0 & -1 & 0 &  3 \\\\\n",
    "             0 & -2 & 1 &  2 \\\\\n",
    "             0 &  3 & 4 & -2 \\\\\n",
    "             0 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                \\color{red}{0} & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "(Notice how changing only one pivot changes only one column of $A$: each column of $U$ determines one column of $A$ via our \"matrix × columns\" viewpoint on matrix multiplication.)\n",
    "\n",
    "Intuitively, having more zero pivots seems \"more singular\", and requires \"more coincidences\" in the right-hand side to have a solution, and has a \"bigger infinity\" of solutions when there *is* a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation matrices\n",
    "\n",
    "As we have seen in the previous section, it is sometimes necessary to rearrange the rows of a matrix when performing elimination.  This row operation can also be done by multiplying the matrix with an elementary matrix.  Let's build a matrix $P$ that performs an exchange of rows 2 and 3 in a $4\\times 4$ matrix.  Again, we can do this by performing the same row operation on the identity matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\n",
      "[[-3 -1  4]\n",
      " [ 3 -1 -4]\n",
      " [-1 -5  1]]\n",
      "\n",
      "P:\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "PC:\n",
      "[[-3. -1.  4.]\n",
      " [-1. -5.  1.]\n",
      " [ 3. -1. -4.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laguide as lag\n",
    "\n",
    "C = np.random.randint(-6,6,size=(3,3))\n",
    "I = np.eye(3)\n",
    "P = lag.RowSwap(I,1,2)\n",
    "\n",
    "print(\"C:\",'\\n',C,'\\n',sep='')\n",
    "print(\"P:\",'\\n',P,'\\n',sep='')\n",
    "print(\"PC:\",'\\n',P@C,sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the row operation is a row swap, it is common to refer to the corresponding elementary matrix as a **permutation matrix**, and use the letter $P$ to represent it.  We will follow this convention.  It is also useful to note that an elementary permutation matrix is its own inverse since the operation of swapping two rows can be reversed by performing the exact same operation.  We can check that $PP=I$, which means that $P=P^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(P@P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Suppose that you are *given* the $PA = LU$ factorization of an invertible $m \\times m$ matrix $A$, and we want to solve the block-matrix equation:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} A & B \\\\ 0 & A \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n",
    "= \\begin{pmatrix} b \\\\ c \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $B$ is an $m \\times m$ matrix, \"0\" denotes an $m \\times m$ block of zeros, and x,y,b,c are m-component vectors.\n",
    "\n",
    "Write the solution vectors x,y in terms of P,L,U,B,b,c (or the inverses of these matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution** \n",
    "\n",
    "The system we're given is *block* upper-triangular, so just like with usual upper-triangular systems let's solve it by backsubstitution.  \n",
    "\n",
    "The bottom block-row gives the equation $Ay = c$.  As $PA = LU$, we have $A = P^{-1}LU$, so the equation reads $P^{-1}LUy = c$, and as $P, L, U$ are invertible we can multiply on the left by $P$, and then by $L^{-1}$, and then by $U^{-1}$ to obtain $$\\boxed{y = U^{-1}L^{-1}Pc}.$$  (Equivalently, $Ay = c \\implies PAy = Pc = LUy \\implies y = U^{-1} L^{-1} c$.)\n",
    "\n",
    "Now we back-substitute.  The first equation reads $Ax + By = b$ so we have $x = A^{-1}(b - By).$  As $y = U^{-1}L^{-1}Pc$ and $A^{-1} = U^{-1}L^{-1}P$, plugging in we get: $$\\boxed{x = U^{-1}L^{-1}P(b - By)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "Given an $m \\times m$ nonsingular matrix $A$, if no row swaps are needed, we can compute the LU factorization $A = LU$ by Gaussian elimination.\n",
    "\n",
    "Now, suppose that we \"augment\" the matrix $A$ by an $m \\times m$ identity matrix $I$, forming the matrix $\\begin{pmatrix} A & I \\end{pmatrix}$.  If we do Gaussian elimination on *this* matrix (again without row swaps), we will get something like:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} A & I \\end{pmatrix} \\to \\begin{pmatrix} U & C \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the first $m$ columns are the same $U$ matrix as before, and the last $m$ columns (from the elimination steps acting on $I$) are some matrix $C$.\n",
    "\n",
    "* Give an explicit formula for $C$ in terms of $L$ and/or $U$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution** \n",
    "\n",
    "Elimination corresponds to multiplying the matrix on the left by some elimination matrices\n",
    "\n",
    "Remember that each of Gaussian elimination, i.e. some row operation, is given by multiplication on the left by some elimination matrix.  Therefore, if we do some sequence of elimination steps to transform $\\begin{pmatrix} A & I\\end{pmatrix}$ into $\\begin{pmatrix} U & C\\end{pmatrix}$, with the first step corresponding to left-multiplication by $E_1$, the second corresponding to left-multiplication by $E_2$, etc., for a total of $k$ steps, then the entire Gaussian elimination process is given by multiplication on the left by the product matrix $E = E_k \\cdots E_1$.  That is to say, we have $$\\begin{pmatrix} U & C\\end{pmatrix} = X\\begin{pmatrix} A & I\\end{pmatrix} = \\begin{pmatrix} EA & EI\\end{pmatrix}.$$  From this we can read off two equations, by equating the left and right blocks on the far left and right sides:\n",
    "\n",
    "1.  $U = EA$\n",
    "2.  $C = EI = E$.\n",
    "\n",
    "But, from class, $E$ (if there are no row swaps) is a lower-triangular matrix that is simply $L^{-1}$, since $EA=U \\implies A = E^{-1} U = LU$, and in fact we defined $L$ precisely as the inverse of the elimination steps.\n",
    "\n",
    "So $C = E = L^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
