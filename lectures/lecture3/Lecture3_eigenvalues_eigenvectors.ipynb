{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Vector norms\n",
    "- Matrix norms\n",
    "- Scalar product\n",
    "- Unitary matrices\n",
    "- LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today \n",
    "\n",
    "- Eigenvalues and eigenvectors \n",
    "- Similar matrices\n",
    "- Eigendecomposition\n",
    "- Matrix Powers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalues: The Key Idea\n",
    "\n",
    "If we can find a solution $x \\ne 0$ to\n",
    "\n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "\n",
    "then, for this vector, the matrix $A$ **acts like a scalar**.  $x$ is called an **eigenvector** of $A$, and $\\lambda$ is called an **eigenvalue**.\n",
    "\n",
    "In fact, for an $m \\times m$ matrix $A$, we typically find $m$ linearly independendent eigenvectors $x_1,x_2,\\ldots,x_m$ and $m$ corresponding eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$.   Such a matrix is called **diagonalizable**.  Most matrices are diagonalizable; we will deal with the rare \"defective\" (non-diagonalizable) case later.\n",
    "\n",
    "Given such a **basis of eigenvectors**, the key idea for using them is:\n",
    "\n",
    "1. Take any vector $x$ and expand it in this basis: $x = c_1 x_1 + \\cdots c_m x_n$, or $x = Xc$ or $c = X^{-1}x$ where $X$ is the matrix whose *columns are the eigenvectors*.\n",
    "\n",
    "2. For each eigenvector $x_k$, the matrix $A$ acts like a scalar $\\lambda_k$.  Multiplication or division corresponds to multiplying/dividing $x_k$ by $\\lambda_k$.  **Solve your problem for each eigenvector by treating A as the scalar λ**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The characteristic polynomial\n",
    "\n",
    "To *find* the eigenvalues, one approach is to realize that $Ax = \\lambda x$ means:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I) x = 0 \\, ,\n",
    "$$\n",
    "\n",
    "therefore matrix $A - \\lambda I$ has **non-trivial kernel** and should be **singular**. \n",
    "\n",
    "That means, that the **determinant**  \n",
    "\n",
    "$$ p(\\lambda) = \\det(A - \\lambda I) = 0. $$\n",
    "\n",
    "where $p(\\lambda)$ is the **characteristic polynomial of A: a polynomial of degree m** if $A$ is $m \\times m$.  \n",
    "\n",
    "The **roots of this polynomial are the eigenvalues λ**.\n",
    "\n",
    "A polynomial of degree $m$ has at most $m$ roots (possibly complex), and typically has $m$ distinct roots.  **This is why most matrices have $m$ distinct eigenvalues/eigenvectors**, and are therefore **diagonalizable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall the determinant\n",
    "The determinant of a square matrix $A$ is defined as \n",
    "\n",
    "$$\\det A = \\sum_{\\sigma \\in S_n} \\mathrm{sgn}({\\sigma})\\prod^n_{i=1} a_{i, \\sigma_i},$$\n",
    "\n",
    "where \n",
    "- $S_n$ is the set of all **permutations** of the numbers $1, \\ldots, n$\n",
    "- $\\mathrm{sgn}$ is the **signature** of the permutation ( $(-1)^p$, where $p$ is the number of transpositions to be made, the sign is negative iff there are an odd number of misordered pairs of indices).\n",
    "\n",
    "<img src=\"determinant.png\" style=\"width: 700px;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of determinant:\n",
    "\n",
    "- $\\det(AB) = \\det(A) \\det(B)$\n",
    "- $\\det A = \\det \\Lambda = \\lambda_1 \\lambda_2 \\cdots \\lambda_m$.  That is, the **determinant is the product of the eigenvalues**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Similar matrices\n",
    "\n",
    "B is **similar** to A if and only if $B = S^{-1} A S$ for some invertible matrix S.\n",
    "\n",
    "It also follows that $A = SBS^{-1} = (S^{-1})^{-1} B (S^{-1})$, i.e. if B is similar to A using S, then A is similar to B using $S^{-1}$.\n",
    "\n",
    "Characteristic polynomial:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = \\det(SBS^{-1} - \\lambda I) = \\det \\left[ S (B - \\lambda I) S^{-1}   \\right]\n",
    "= \\det(S) \\det(B - \\lambda I) \\det(S^{-1}) = \\det(B - \\lambda I)\n",
    "$$\n",
    "\n",
    "i.e. **similar matrices have the same characteristic polynomial**.\n",
    "\n",
    "$\\det A = \\det B$, i.e. **similar matrices have the same determinant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trace = Sum of λ’s\n",
    "The trace is defined as the **sum of the diagonal elements** of any matrix.  By plugging in the definition of matrix multiplication, one can quickly show that the trace has a crucial property:\n",
    "\n",
    "$$\n",
    "\\operatorname{trace} (AB) = \\operatorname{trace}(BA)\n",
    "$$\n",
    "\n",
    "\n",
    "It follows that **similar matrices have the same trace**, since if $A=SBS^{-1}$ then \n",
    "\n",
    "$$\n",
    "\\operatorname{trace} (A) = \\operatorname{trace}(SBS^{-1}) = \\operatorname{trace}(S^{-1}SB) = \\operatorname{trace}(B)\n",
    "$$\n",
    "\n",
    "In particular, since A and Λ are similar, this means that the **trace of a matrix is the sum of the eigenvalues**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvalue example\n",
    "\n",
    "For example, consider the matrix\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 1 \\\\ -2 & 4 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The characteristic polynomial is\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = \\det \\begin{pmatrix} 1 - \\lambda & 1 \\\\ -2 & 4 - \\lambda \\end{pmatrix} = (1 - \\lambda)(4 - \\lambda) - (1)(-2) = \\lambda^2 - 5\\lambda + 6 = (\\lambda - 2) (\\lambda - 3)\n",
    "$$\n",
    "\n",
    "where we have used high-school algebra to factor the polynomial.   Hence its roots are $\\lambda = \\{2, 3\\}$, as computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigenvectors\n",
    "\n",
    "Once we have the eigenvalues, finding the eigenvectors is (in principle) easy: **the eigenvectors are just a basis for the nullspace** of\n",
    "\n",
    "$$\n",
    "A - \\lambda I\n",
    "$$\n",
    "\n",
    "when $\\lambda$ is an eigenvalue.\n",
    "\n",
    "For example, with the matrix above, let's take the eigenvalue $\\lambda_1 = 2$:\n",
    "\n",
    "$$\n",
    "A - 2I = \\begin{pmatrix} -1 & 1 \\\\ -2 & 2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We could go through Gaussian elimination to find the nullspace, but we can see by inspection that the second column is minus the first, hence $x_1 = (1, 1)$ is a basis for the nullspace:\n",
    "\n",
    "$$\n",
    "(A - 2I) x_1 = \\begin{pmatrix} -1 & 1 \\\\ -2 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "A x_1 = 2 x_1\n",
    "$$\n",
    "\n",
    "as desired.  $x_1 = (1, 1)$ is an eigenvector!\n",
    "\n",
    "For the other eigenvalue, $\\lambda = 3$, we get:\n",
    "\n",
    "$$\n",
    "A - 3I = \\begin{pmatrix} -2 & 1 \\\\ -2 & 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "from which it is obvious that a basis for the nullspace is $x_2 = (1, 2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complex eigenvalues\n",
    "\n",
    "If we change the matrix to:\n",
    "$$\n",
    "\\begin{pmatrix} 1 & 3 \\\\ -2 & 4 \\end{pmatrix}\n",
    "$$\n",
    "we get a characteristic polynomial:\n",
    "$$\n",
    "\\det \\begin{pmatrix} 1 - \\lambda & 3 \\\\ -2 & 4 - \\lambda \\end{pmatrix} = (1 - \\lambda)(4 - \\lambda) - (3)(-2) = \\lambda^2 - 5\\lambda + 10\n",
    "$$\n",
    "whose roots, from the quadratic formula, are:\n",
    "$$\n",
    "\\lambda = \\frac{5 \\pm \\sqrt{5^2 - 40}}{2} = \\frac{5 \\pm \\sqrt{-15}}{2}\n",
    "$$\n",
    "which are complex!\n",
    "\n",
    "**Eigenvalues may be complex numbers, even for real matrices**. \n",
    "\n",
    "For real matrices, they are the [roots of a real polynomial](https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem) and hence come in [complex conjugate pairs](https://en.wikipedia.org/wiki/Complex_conjugate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing eigenvalues = polynomial roots = hard\n",
    "\n",
    "* Everyone learns the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula) to find roots of a quadratic (degree-2) polynomial.\n",
    "\n",
    "* There is a (horrible) [cubic formula](https://en.wikipedia.org/wiki/Cubic_function) to find the roots of any cubic (degree-3) polynomial.\n",
    "\n",
    "* There is a (terrifying) [quartic formula](https://en.wikipedia.org/wiki/Quartic_function) to find the roots of any quartic (degree-4) polynomial.\n",
    "\n",
    "* There is **no formula** (in terms of a *finite number* of ±,×,÷,ⁿ√) for the roots of an **arbitrary quintic** polynomial or **any degree ≥ 5**.  This is the [Abel–Ruffini theorem](https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem), proved in the 19th century."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, computing eigenvalues by hand, especially by this method, is even more pointless than doing Gaussian elimination by hand, for reasons explained below, so I will **focus more on the properties of eigenvalues and how to use them than how to compute them.**  The computer will give us their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transpose: Same eigenvalues!\n",
    "\n",
    "One of the properties of determinant is that $\\det A^T = \\det A$.   It follows that\n",
    "$$\\det(A-\\lambda I) = \\det\\left[ (A -\\lambda I)^T \\right] = \\det (A^T - \\lambda I)$$\n",
    "and therefore $A$ and $A^T$ have the **same eigenvalues!**  (They have the **same characteristic polynomial**.)\n",
    "\n",
    "However, $A$ and $A^T$ in general have **different eigenvectors**. \n",
    "\n",
    "$N(A - \\lambda I) \\ne N(A^T - \\lambda I)$ in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "If matrix $A$ of size $n\\times n$ has $n$ eigenvectors $s_i$, $i=1,\\dots,n$:\n",
    "\n",
    "$$ As_i = \\lambda_i s_i, $$\n",
    "\n",
    "then this can be written as\n",
    "\n",
    "$$ A S = S \\Lambda, \\quad\\text{where}\\quad S=(s_1,\\dots,s_n), \\quad \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n), $$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$ A = S\\Lambda S^{-1}. $$\n",
    "\n",
    "- This is called **eigendecomposition** of a matrix. Matrices that can be represented by their eigendecomposition are called **diagonalizable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagonalization and Matrix Powers\n",
    "\n",
    "We've already seen that if $Ax = \\lambda x$ then $A^n x = \\lambda^n x$ (for both positive and negative $n$): **if x is an eigenvector of A, then it is also an eigenvector of Aⁿ with the eigenvalue raised to the n-th power**.\n",
    "\n",
    "There is another cute way to see this for diagonalizable matrices.  If $A = X\\Lambda X^{-1}$, then for $n \\ge 0$\n",
    "\n",
    "$$\n",
    "A^n = \\underbrace{AAA\\cdots A}_{n\\mbox{ times}} \n",
    "    = \\underbrace{X\\Lambda X^{-1}X\\Lambda X^{-1}X\\Lambda X^{-1}\\cdots X\\Lambda X^{-1}}_{n\\mbox{ times}} = X\\Lambda^n X^{-1}\n",
    "$$\n",
    "because all of the $X$ terms *cancel* except the first and last ones.  $\\Lambda^n$ is just the diagonal matrix with $\\lambda_1^n, \\lambda_2^n, \\ldots$ on the diagonal.\n",
    "\n",
    "So, since we have the diagonalization of $A^n$, we immediately see that its eigenvectors are $X$ (same as for $A$) and its eigenvalues are $\\lambda^n$.\n",
    "\n",
    "Since $A^{-1}x = \\lambda^{-1} x$ for an eigenvector $x$, it immediately follows that $A^{-1} = X \\Lambda^{-1} X^{-1}$ where $\\Lambda^{-1}$ is the diagonal matrix with $\\lambda_k^{-1}$ on the diagonal.  Similarly for $A^{-n} = X \\Lambda^{-n} X^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix square roots\n",
    "\n",
    "One really cool thing that diagonalization allows us to do easily is to compute $A^n$ for **non-integer powers n**.   For example, we can now see how to find the [square root of a matrix](https://en.wikipedia.org/wiki/Square_root_of_a_matrix), at least for diagonalizable matrices.\n",
    "\n",
    "If $A$ is a square matrix, its square root $\\sqrt{A} = A^{1/2}$ is just a matrix so that $A^{1/2} A^{1/2} = A$.  But how would we find such a matrix?\n",
    "\n",
    "As usual, for eigenvalues it is easy: if $Ax=\\lambda x$, then we obviously want $A^{1/2} x = \\lambda^{1/2} x$.  If $A$ is diagonalizable and we do this for *every* eigenvalue/vector, we get the diagonalization:\n",
    "\n",
    "$$\n",
    "\\sqrt{A} = A^{1/2} = X \\underbrace{\\begin{pmatrix} \\sqrt{\\lambda_1} & & & \\\\ & \\sqrt{\\lambda_2} & & \\\\ & & \\ddots & \\\\ & & & \\sqrt{\\lambda_m} \\end{pmatrix}}_\\sqrt{\\Lambda} X^{-1}\n",
    "$$\n",
    "(Obviously, this may give a complex matrix if any eigenvalues are negative or complex.)\n",
    "\n",
    "Does this have the property that $A^{1/2} A^{1/2} = A$? Yes!  $X \\sqrt{\\Lambda} X^{-1} X \\sqrt{\\Lambda} X^{-1} = X \\sqrt{\\Lambda} \\sqrt{\\Lambda} X^{-1} = A$, since obviously $\\sqrt{\\Lambda} \\sqrt{\\Lambda} = \\Lambda$ from the definition above.\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Derivatives\n",
    "- Partial derivatives\n",
    "- Free maxima and minima "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
