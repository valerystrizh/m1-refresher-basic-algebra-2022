{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Vector norms, matrix norms, LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "- Vector spaces \n",
    "- Matrices\n",
    "- Matvecs and matmuls\n",
    "- Linear systems\n",
    "- Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today \n",
    "\n",
    "- Vector norms\n",
    "- Matrix norms\n",
    "- Scalar product\n",
    "- Unitary matrices\n",
    "- LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "\n",
    "- Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "Properties of norm:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality)\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "\n",
    "$$ d(x, y) = \\Vert x - y \\Vert. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "\n",
    "which corresponds to the distance in our real life. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "\n",
    "$$ \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}. $$\n",
    "\n",
    "- Infinity norm is defined as the maximal element: \n",
    "\n",
    "$$ \\Vert x \\Vert_{\\infty} = \\max_i | x_i| $$\n",
    "\n",
    "\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: \n",
    "\n",
    "$$ \\Vert x \\Vert_1 = \\sum_i |x_i| $$\n",
    "  \n",
    "<img src=\"manhattan.jpeg\" style=\"width: 200px;\">  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "\n",
    "$$ C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_* $$  \n",
    "\n",
    "for some positive constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. \n",
    "\n",
    "The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. \n",
    "\n",
    "However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix norms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\Vert \\cdot \\Vert$ is called a **matrix norm** if it is a vector norm on the vector space of $n \\times m$ matrices:\n",
    "1. $\\|A\\| \\geq 0$ and if $\\|A\\| = 0$ then $A = O$\n",
    "3. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
    "4. $\\|A+B\\| \\leq \\|A\\| + \\|B\\|$ (triangle inequality)\n",
    "\n",
    "Additionally some norms can satisfy the *submultiplicative property*\n",
    "\n",
    "* <font color='red'> $\\Vert A B \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert$ </font>\n",
    "\n",
    "- These norms are called **submultiplicative norms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operator norms\n",
    "\n",
    "- The most important class of the matrix norms is the class of **operator norms**. They are defined as\n",
    "\n",
    "$$ \\Vert A \\Vert_{*,**} = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_*}{\\Vert x \\Vert_{**}}, $$\n",
    "\n",
    "where $\\Vert \\cdot \\Vert_*$ and $\\| \\cdot \\|_{**}$ are **vector norms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix $p$-norms\n",
    "\n",
    "Important case of operator norms are matrix $p$-norms, which are defined for $\\|\\cdot\\|_* = \\|\\cdot\\|_{**} = \\|\\cdot\\|_p$. <br>\n",
    "\n",
    "Among all $p$-norms three norms are the most common ones:  \n",
    "\n",
    "- $p = 1, \\quad \\Vert A \\Vert_{1} = \\displaystyle{\\max_j \\sum_{i=1}^n} |a_{ij}|$.\n",
    "\n",
    "- $p = 2, \\quad$ spectral norm, denoted by $\\Vert A \\Vert_2$.\n",
    "\n",
    "- $p = \\infty, \\quad \\Vert A \\Vert_{\\infty} = \\displaystyle{\\max_i \\sum_{j=1}^m} |a_{ij}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frobenius norm\n",
    "$$ \\Vert A \\Vert_F \\stackrel{\\mathrm{def}}{=} \\Big(\\sum_{i=1}^n \\sum_{j=1}^m |a_{ij}|^2\\Big)^{1/2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral norm\n",
    "\n",
    "- Spectral norm, $\\Vert A \\Vert_2$ is one of the most used matrix norms (along with the Frobenius norm). \n",
    "- It can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it.  \n",
    "- It is directly related to the **singular value decomposition** (SVD) of the matrix. It holds\n",
    "\n",
    "$$ \\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_\\max(A^*A)} $$\n",
    "\n",
    "where $\\sigma_1(A)$ is the largest singular value of the matrix $A$ and $^*$ is a *conjugate transpose* of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral: 54.15852109368699 \n",
      "Frobenius: 446.8592975536342 \n",
      "1-norm: 100.33618257395729 \n",
      "infinity: 1660.5271740463759\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 2000\n",
    "a = np.random.randn(n, m) #Random n x n matrix\n",
    "s1 = np.linalg.norm(a, 2) #Spectral\n",
    "s2 = np.linalg.norm(a, 'fro') #Frobenius\n",
    "s3 = np.linalg.norm(a, 1) #1-norm\n",
    "s4 = np.linalg.norm(a, np.inf) \n",
    "print('Spectral: {0:} \\nFrobenius: {1:} \\n1-norm: {2:} \\ninfinity: {3:}'.format(s1, s2, s3, s4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "Several examples of optimization problems where matrix norms arise:\n",
    "* $\\displaystyle{\\min_{\\mathrm{rank}(A_r)=r}}\\| A - A_r\\|$  –– finding best rank-r approximation. SVD helps to solve this problem for $\\|\\cdot\\|_2$ and $\\|\\cdot\\|_F$.\n",
    "\n",
    "\n",
    "* $\\displaystyle{\\min_{B,C\\geq 0}} \\|A - BC\\|_F$ –– nonnegative matrix factorization. Symbol $B\\geq0$ here means that all elements of $B$ are nonnegative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalar product\n",
    "While norm is a measure of distance, the **scalar product** takes angle into account.  \n",
    "\n",
    "It is defined as\n",
    "\n",
    "* **For vectors:**\n",
    "$$ (x, y) =  x^* y = \\sum_{i=1}^n \\overline{x}_i y_i, $$\n",
    "where $\\overline{x}$ denotes the *complex conjugate* of $x$. The Euclidean norm is then\n",
    "\n",
    "$$ \\Vert x \\Vert_2 = \\sqrt{(x, x)}, $$\n",
    "\n",
    "or it is said the norm is **induced** by the scalar product.  \n",
    "\n",
    "\n",
    "* **For matrices** (Frobenius scalar product):\n",
    "\n",
    "$$ (A, B)_F = \\displaystyle{\\sum_{i=1}^{n}\\sum_{j=1}^{m}} \\overline{a}_{ij} b_{ij} \\equiv \\mathrm{trace}(A^* B), $$\n",
    "\n",
    "where $\\mathrm{trace}(A)$ denotes the sum of diagonal elements of $A$. One can check that $\\|A\\|_F = \\sqrt{(A, A)_F}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Remark**. The angle between two vectors is defined as\n",
    "\n",
    "$$ \\cos \\phi = \\frac{(x, y)}{\\Vert x \\Vert_2 \\Vert y \\Vert_2}. $$\n",
    "\n",
    "Similar expression holds for matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- An important property of the scalar product is the **Cauchy-Schwarz-Bunyakovski inequality**:\n",
    "\n",
    "$$|(x, y)| \\leq \\Vert x \\Vert_2 \\Vert y \\Vert_2,$$\n",
    "\n",
    "and thus the angle between two vectors is defined properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary (orthogonal) matrices\n",
    "\n",
    "- Let $U$ be complex $n \\times n$ matrix, and $\\Vert U z \\Vert_2 = \\Vert z \\Vert_2$ for all $z$. \n",
    "\n",
    "- This can happen iff (if and only if)\n",
    "\n",
    "$$ U^* U = I_n, $$\n",
    "\n",
    "where $I_n$ is an identity matrix $n\\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Complex $n\\times n$ square matrix is called **unitary** if\n",
    "\n",
    "$$ U^*U = UU^* = I_n, $$\n",
    "\n",
    "which means that columns and rows of unitary matrices both form orthonormal basis in $\\mathbb{C}^{n}$.\n",
    "\n",
    "- For rectangular matrices of size $m\\times n$ ($n\\not= m$) only one of the equalities can hold\n",
    "\n",
    "    - $ U^*U = I_n$ –– left unitary for $m>n$\n",
    "    - $ UU^* = I_m$  –– right unitary for $m<n$\n",
    "\n",
    "- In the case of real matrices $U^* = U^T$ and matrices such that\n",
    "\n",
    "$$ U^TU = UU^T = I $$\n",
    "\n",
    "are called **orthogonal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unitary matrices\n",
    "\n",
    "Important property: **a product of two unitary matrices is a unitary matrix:**  \n",
    "\n",
    "$$(UV)^* UV = V^* (U^* U) V = V^* V = I,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "SVD will be considered in the Machine Learning course in more details.\n",
    "\n",
    "**Theorem.** Any matrix $A\\in \\mathbb{C}^{n\\times m}$ can be written as a product of three matrices:  \n",
    "\n",
    "$$ A = U \\Sigma V^*, $$\n",
    "\n",
    "where \n",
    "- $U$ is an $n \\times n$ unitary matrix\n",
    "- $V$ is an $m \\times m$ unitary matrix\n",
    "- $\\Sigma$ is a diagonal matrix with non-negative elements $\\sigma_1 \\geq  \\ldots, \\geq \\sigma_{\\min (m,n)}$ on the diagonal.\n",
    "\n",
    "Moreover, if $\\text{rank}(A) = r$, then $\\sigma_{r+1} = \\dots = \\sigma_{\\min (m,n)} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian elimination and LU decomposition\n",
    "\n",
    "- Gaussian elimination is the computation of one of the most important matrix decompositions: **LU-decomposition**.\n",
    "\n",
    "**Definition**: LU-decomposition of the square matrix $A$ is the representation\n",
    "\n",
    "$$A =  LU,$$\n",
    "\n",
    "where \n",
    "- $L$ is **lower triangular** (elements strictly above the diagonal are zero)\n",
    "- $U$ is **upper triangular** matrix (elements strictly below the diagonal are zero)\n",
    "\n",
    "This factorization is **non-unique**, so it is typical to require that the matrix $L$ has ones on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing LU-decomposition\n",
    "\n",
    "- Once the decomposition is found (it costs $\\mathcal{O}(n^3)$ operations), then solving linear systems with $L$ and $U$ costs only $\\mathcal{O}(n^2)$ operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Main goal** of the LU decomposition is to solve linear system, because\n",
    "\n",
    "$$ A^{-1} f = (L U)^{-1} f = U^{-1} L^{-1} f, $$\n",
    "\n",
    "and this reduces to the solution of two linear systems **forward step**\n",
    "\n",
    "$$ L y = f, $$\n",
    "\n",
    "and **backward step**\n",
    "\n",
    "$$ U x = y. $$\n",
    "\n",
    "Does $LU$ decomposition always exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When LU fails\n",
    "\n",
    "- What happens, if the matrix is not strictly regular (or the **pivots** in the Gaussian elimination are really small?). \n",
    "\n",
    "- There is classical $2 \\times 2$ example of a matrix with a bad LU decomposition.  \n",
    "\n",
    "- The matrix we look at is  \n",
    "\n",
    "$$\n",
    "    A = \\begin{pmatrix}\n",
    "    \\varepsilon & 1 \\\\\n",
    "    1 & 1 \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- If $\\varepsilon$ is sufficiently small, we **might** fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The concept of pivoting\n",
    "\n",
    "- We can do pivoting, i.e. permute rows and columns to maximize $A_{kk}$ that we divide over.  \n",
    "\n",
    "- The simplest but effective strategy is the **row pivoting**: at each step, select the index that is maximal in modulus, and put it onto the diagonal. \n",
    "\n",
    "- It gives us the decomposition \n",
    "\n",
    "$$A = P L U,$$\n",
    "\n",
    "where $P$ is a **permutation matrix**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Eigenvectors and eigenvalues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
