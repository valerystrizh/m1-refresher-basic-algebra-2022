{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often conceptually convenient to talk about the *inverse* $A^{-1}$ of a matrix $A$, which exists **for any non-singular square matrix**.   This is the matrix such that $x = A^{-1}b$ solves $Ax = b$ for any $b$.   The inverse is conceptually convenient becuase it allows us to move matrices around in equations *almost* like numbers (except that matrices don't commute!).\n",
    "\n",
    "Another way of defining the inverse of a matrix involves the *identity* matrix $I$.   Here is a $ 5 \\times 5$ identity matrix:\n",
    "\n",
    "$$\n",
    "I = \\begin{pmatrix} 1&0&0&0&0 \\\\ 0&1&0&0&0 \\\\ 0&0&1&0&0 \\\\ 0&0&0&1&0 \\\\ 0&0&0&0&1 \\end{pmatrix}\n",
    "= \\begin{pmatrix} e_1 & e_2 & e_3 & e_4 & e_5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the columns $e_1 \\cdots e_5$ of $I$ are the **unit vectors** in each component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse matrix $A^{-1}$ is the matrix such that $A^{-1} A = A A^{-1} = I$.\n",
    "\n",
    "Why does this correspond to solving $Ax=b$?  Multiplying both sides on the *left* by $A^{-1}$ (multiplying on the *right* would make no sense: we can't multiply vector√ómatrix!), we get \n",
    "\n",
    "$$\n",
    "A^{-1}Ax=Ix = x = A^{-1}b\n",
    "$$\n",
    "\n",
    "How do we find $A^{-1}$?  The key is the equation $A A^{-1} = I$, which looks just like $AX=B$ for the **right-hand sides consisting of the columns of the identity matrix**, i.e. the unit vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, $AB \\ne BA$ for two matrices $A$ and $B$.  Why can we multiply $A$ by $A^{-1}$ on either the left or right and get the same answer $I$?  It is fairly easy to see why:\n",
    "\n",
    "$$\n",
    "A A^{-1} = I \\implies A A^{-1} A = IA = A = A (A^{-1} A)\n",
    "$$\n",
    "\n",
    "Since $A (A^{-1} A) = A$, and $A$ is non-singular (so there is a unique solution to this system of equations), we must have $A^{-1} A = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix inverses are funny, however:\n",
    "\n",
    "* Inverse matrices are very convenient in *analytical* manipulations, because they allow you to move matrices from one side to the other of equations easily.\n",
    "\n",
    "* Inverse matrices are **almost never computed** in \"serious\" numerical calculations.  Whenever you see $A^{-1} B$ (or $A^{-1} b$), when you go to *implement* it on a computer you should *read* $A^{-1} B$ as \"solve $AX = B$ by some method.\" e.g. solve it by `A \\ B` or by first computing the LU factorization of $A$ and then using it to solve $AX = B$.\n",
    "\n",
    "One reason that you don't usually compute inverse matrices is that it is wasteful: once you have $PA=LU$, you can solve $AX=B$ directly without bothering to find $A^{-1}$, and computing $A^{-1}$ requires much more work if you only have to solve a few right-hand sides.\n",
    "\n",
    "Another reason is that for many special matrices, there are ways to solve $AX=B$ *much* more quickly than you can find $A^{-1}$.   For example, many large matrices in practice are [sparse](https://en.wikipedia.org/wiki/Sparse_matrix) (mostly zero), and often for sparse matrices you can arrange for $L$ and $U$ to be sparse too.  Sparse matrices are much more efficient to work with than general \"dense\" matrices because you don't have to multiply (or even store) the zeros. Even if $A$ is sparse, however, $A^{-1}$ is usually non-sparse, so you lose the special efficiency of sparsity if you compute the inverse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about inverse matrices:\n",
    "\n",
    "1. The matrix must be square in order for this definition to make sense.  If $A$ is not square, it is impossible for both \n",
    "$A^{-1}A$ and $AA^{-1}$ to be defined.\n",
    "2. Not all matrices have inverses.  Matrices that do have inverses are called **invertible** matrices.  Matrices that do not have inverses are called **non-invertible**, or **singular**, matrices.\n",
    "3. If a matrix is invertible, its inverse is unique.\n",
    "\n",
    "Now *if we know* $A^{-1}$, we can solve the system $AX=B$ by multiplying both sides by $A^{-1}$.\n",
    "\n",
    "$$\n",
    "A^{-1}AX = A^{-1}B\n",
    "$$\n",
    "\n",
    "Then $A^{-1}AX = IX = X$, so the solution to the system is $X=A^{-1}B$.  Unfortunately, it is typically not easy to find $A^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverses and products\n",
    "\n",
    "Inverses have a special relationship to matrix products:\n",
    "$$\n",
    "(AB)^{-1} = B^{-1} A^{-1}\n",
    "$$\n",
    "\n",
    "The reason for this is that we must have $(AB)^{-1} AB = I$, and it is easy to see that $B^{-1} A^{-1}$ does the trick.  Equivalently, $AB$ is the matrix that first multiplies by $B$ then by $A$; to invert this, we must *reverse the steps*: first multiply by the inverse of $A$ and then by the inverse of $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity of Matrix Operations\n",
    "\n",
    "With a little effort, we can figure out that the **number of arithmetic operations** for an $n\\times n$ matrix **scales proportional to** (for large $n$):\n",
    "\n",
    "* $n^2$ for: matrix `*` vector $Ax$, or solving a *triangular* system like $Ux=c$ or $Lc=b$ (back/forward substitution)\n",
    "* $n^3$ for: matrix `*` matrix $AB$, LU factorization $PA=LU$, or solving a triangular system with $n$ right-hand sides like computing $A^{-1}$ from the LU factorization.\n",
    "\n",
    "(In computer science, we would say that these have \"complexity\" $\\Theta(n^2)$ and $\\Theta(n^3)$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose, Permutations, and Orthogonality\n",
    "\n",
    "One special type of matrix for which we can solve problems much more quickly is a permutation matrix, introduced in the lecture on $PA=LU$ factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of any permutation matrix $P$ turns out to be its [transpose](https://en.wikipedia.org/wiki/Transpose) $P^T$: we just swap rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this works is that $P^T P$ computes the dot products of *all the columns* of $P$ with *all of the columns*, and the columns of $P$ are [orthonormal](https://en.wikipedia.org/wiki/Orthonormality) (orthogonal with length 1).  We say that $P$ is an example of an [\"orthogonal\" matrix or a \"unitary\" matrix](https://en.wikipedia.org/wiki/Unitary_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposes and products\n",
    "\n",
    "Transposes are important in linear algebra because they have a special relationship to matrix and vector products:\n",
    "$$\n",
    "(AB)^T = B^T A^T\n",
    "$$\n",
    "and hence for a dot product (inner product) $x^T y$\n",
    "$$\n",
    "x \\mbox{ dot } (Ay) = x^T (Ay) = (A^T x)^T y = (A^T x) \\mbox{ dot } y\n",
    "$$\n",
    "We can even turn the second step around and use this as the *definition* of a transpose: a transpose is *what \"moves\" a matrix from one side to the other of a dot product.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposes and inverses\n",
    "\n",
    "From the above property, we have:\n",
    "$$\n",
    "(A A^{-1})^T = (A^{-1})^T A^T = I^T = I\n",
    "$$\n",
    "and it follows that:\n",
    "$$\n",
    "(A^{-1})^T = (A^T)^{-1}\n",
    "$$\n",
    "The *transpose of the inverse* is the *inverse of the transpose*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Matrix Inverses in General\n",
    "\n",
    "How do we find $A^{-1}$?  The key is the equation $A A^{-1} = I$, which looks just like $AX=B$ for the **right-hand sides consisting of the columns of the identity matrix**, i.e. the unit vectors.   So, we just solve $Ax=e_i$ for $i=1,\\ldots,5$.\n",
    "\n",
    "We take $C$ as an example matrix, and consider how we might build the inverse.\n",
    "\n",
    "$$\n",
    "C = \\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Let's think of the matrix product $CC^{-1}= I$ in terms of the columns of $C^{-1}$.  We put focus on the third column as an example, and label those unknown entries with $y_i$.  The \\* entries are uknown as well, but we will ignore them for the moment.\n",
    "\n",
    "$$\n",
    "CC^{-1}=\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{rrrr} * & * & y_1& * \\\\ * & * & y_2 & * \\\\ * & * & y_3 & * \\\\ * & * & y_4 & *  \\end{array}\\right]=\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{array}\\right]=\n",
    "I\n",
    "$$\n",
    "\n",
    "Recall now that $C$ multiplied by the third column of $C^{-1}$ produces the third column of $I$.  This gives us a linear system to solve for the $y_i$.\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r}  y_1 \\\\  y_2  \\\\ y_3 \\\\ y_4  \\end{array}\\right]=\n",
    "\\left[ \\begin{array}{r}  0 \\\\  0  \\\\ 1 \\\\ 0  \\end{array}\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse matrices with SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.33333333e-01  5.00000000e-01 -1.66666667e-01 -5.00000000e-01]\n",
      " [-2.08333333e+00 -1.25000000e+00  6.66666667e-01  2.25000000e+00]\n",
      " [-8.33333333e-02 -2.50000000e-01  1.66666667e-01  2.50000000e-01]\n",
      " [-3.33333333e-01 -6.16790569e-18  1.66666667e-01  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sla\n",
    "\n",
    "C = np.array([[1,0,2,-1],[3,1,-3,2],[2,0,4,4],[2,1,-1,-1]])\n",
    "C_inverse = sla.inv(C)\n",
    "print(C_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing a non-invertible matrix to $\\texttt{inv}$ will result in an error being raised by the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *symmetric* matrix $S$ is a matrix that equals its transpose: $S = S^T$.   Suppose that $A=A^T$ and $B=B^T$ are two symmetric matrices, and $C$ is some other matrix (possibly nonsymmetric), all of the same size $m\\times m$.  Which of the following are certainly symmetric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "* $A^2 - B^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "* $(A+B)(A-B)$ (note that this does *not* generally equal $A^2 - B^2$ ... why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "* $C^T A C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "* $A^T C A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:**\n",
    "* $ABA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**\n",
    "* $C + C^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**\n",
    "* $C - C^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:**\n",
    "* $ABAB$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution** \n",
    "\n",
    "The idea is to just take the transpose in each case and see what you get.  If you get the same thing, the matrix in question was symmetric.  If you don't get the same form, you can usually see why, and you can then cook up some choices of the matrices that really show the result doesn't have to be symmetric.\n",
    "\n",
    "The calculations below use repeatedly the facts that $(X + Y)^T = X^T + Y^T$ and $(XY)^T = Y^TX^T$.  We also of course use the facts that $A^T = A$ and $B^T = B$ each time.\n",
    "\n",
    "* $A^2 - B^2$: MUST be symmetric. Taking transposes, we get $$(A^2 - B^2)^T = (AA)^T - (BA)^T = A^TA^T - B^TB^T = A^2 - B^2$$\n",
    "* $(A+B)(A-B)$: DOES NOT need to be symmetric.  First, multiplying out the expression we get $(A^2 - B^2) + (BA - AB)$.  This *does not have to equal* $A^2 - B^2$ because it's possible that $AB \\neq BA$ (i.e. when $A$ and $B$ don't *commute*.  Taking the transpose gives: $$(A^2 - B^2)^T + (BA - AB)^T = A^2 - B^2 - A^TB^T - B^TAT^ = A^2 - B^2 - (BA - AB).$$  So, we see that $(A + B)(A - B)$ is symmetric if and only if $BA - AB = AB - BA$, i.e. if and only if $AB = BA$, i.e when $A$ and $B$ commute.  We can find symmetric matrices $A$ and $B$ that don't commute pretty easily, for example using our knowledge of row/column operations.  Remember that row operations are achieved by left multiplications, and column operations by right multiplications.  So, if we take $A$ to be the symmetric permutation matrix $$A = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0\\end{pmatrix}$$ then $AB$ will be $B$ with its rows swapped, and $BA$ will be $B$ with its columns swapped.  So, choosing $B$ to be a symmetric matrix that looks different if we swap its rows or columns, for example $$B = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2\\end{pmatrix}$$ we get a concrete counterexample.\n",
    "\n",
    "  - **Remark** The matrix $AB - BA$ is called the [*commutator*](https://en.wikipedia.org/wiki/Commutator) of $A$ and $B$, and measures in a sense how/if $A$ and $B$ commute.  It's an extremely important construction that appears all over the place, for example in making precise the [Heisenberg uncertainty principle](https://en.wikipedia.org/wiki/Uncertainty_principle)\n",
    "\n",
    "* $C^T A C$ MUST be symmetric.  Taking transposes, we get: $$(C^TAC)^T = C^TA^TC = C^TAC$$ which is what we started with.\n",
    "* $A^T C A$ DOES NOT need to be symmetric.  Taking transposes we get $$(A^TCA)^T = A^TC^TA$$ but we don't have to have that $C = C^T$. In fact, if $A$ is invertible, then so is $A^{-1}$, so we can \"cancel\" the $A^T$ on the left and the $A$ on the right when comparing $(A^TCA)^T = A^TC^TA$ and $A^TCA$, obtaining $C = C^T$, so $A^TCA$ is symmetric if and only if $C$ is symmetric (again, when $A$ is invertible).  So choosing $A = I$ and $C$ anything non-symmetric will produce a non-symmetric matrix $A^TCA$.\n",
    "* $ABA$ MUST be symmetric.  Taking transposes, we get $$(ABA)^T = A^TB^TA^T = ABA,$$ the same as we started with.  More generally, any [palindrome](https://en.wikipedia.org/wiki/Palindrome) product of symmetric matrices will still be symmetric.\n",
    "* $C + C^T$ MUST be symmetric.  Taking transposes, we get $$(C + C^T) = C^T + C = C + C^T,$$ the same as we started with.\n",
    "* $C - C^T$ DOES NOT need to be symmetric.  Taking transposes, we get $$(C - C^T)^T = C^T - C = -(C - C^T),$$ the *negative* of what we started with.  The only way a matrix can be equal to its negative is if it was already $0$; so the only way $C - C^T$ can be symmetric is if it was zero already, i.e. if $C = C^T$, i.e. if $C$ was symmetric.  So any non-symmetric $C$ will give a counterexample.\n",
    "\n",
    "  - **Remark** A matrix $X$ that satisfies $X^T = -X$ is called *antisymmetric*.  It's a basic fact that any matrix can be written uniquely as a sum of a symmetric matrix with an antisymmetric matrix.  Indeed, if $X$ is any square matrix, we can write $$X = \\frac{X + X^T}{2} + \\frac{X - X^T}{2}$$ and the first term is symmetric (by two bullet points up) and the second term is anti-symmetric (by one bullet point up).  The uniqueness can be seen like this.  Suppose $X = Y + Z = Y' + Z'$ where $Y, Y'$ are symmetric and $Z, Z'$ are antisymmetric.  Then $Y - Y' = Z - Z'$.  But as $Y, Y'$ are symmetric, so is $Y - Y'$.  As $Z, Z'$ are antisymmetric, so is $Z - Z'$.  But $Y - Y'$ and $Z - Z'$ are equal, and the only matrix that is simultaneously symmetric and antisymmetric is the 0 matrix; so $Y - Y' = Z - Z' = 0$, so $Y = Y$ and $Z = Z'$ and the decompositions $X = Y + Z = Y' + Z'$ were the same to begin with!  As a final note, if we think of a $m \\times m$ matrix as a vector of length $m^2$, then any symmetric matrix is orthogonal to any antisymmetric matrix (why?), so the above actually shows that the *subspaces* of symmetric and anti-symmetric matrices are orthogonal complements of one another.\n",
    "\n",
    "* $ABAB$ DOES NOT need to be symmetric.  Taking transposes, we get $$(ABAB)^T = B^TA^TB^TA^T = BABA$$ so for $ABAB$ to by symmetric means that $ABAB = BABA$.  We can cook up a counterexample using a similar strategy to what we did in the $(A - B)(A + B)$ problem.  In particular, let's just choose some random matrices $A, B$ that are symmetric; they probably won't commute very well, and so $ABAB$ and $BABA$ should be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
