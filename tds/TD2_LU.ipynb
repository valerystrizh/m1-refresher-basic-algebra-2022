{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the last lecture that we were looking at the Gaussian-elimination steps to convert a matrix $A$ into **upper-triangular form** via a sequence of **row operations**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "1 & 1 & -1  \\\\\n",
    "3 & 11 & 6 \n",
    "\\end{pmatrix}}_A\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 2 & 3 \n",
    "\\end{pmatrix}}_{E_1 A}\\to\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "\\boxed{1} & 3 & 1  \\\\\n",
    "0 & \\boxed{-2} & -2  \\\\\n",
    "0 & 0 & \\boxed{1} \n",
    "\\end{pmatrix}}_{E_2 E_1 A}\n",
    "$$\n",
    "\n",
    "Since row operations correspond to **multiplying by matrices on the *left***, constructed the \"elimination matrices\" $E_1$ and $E_2$ corresponding to each of these elimination steps (under the first and second pivots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we arrived at the formula:\n",
    "$$\n",
    "\\underbrace{E_2 E_1}_E A = U\n",
    "$$\n",
    "Notice that we multiplied $A$ by the elimination matrices from *right to left* in the order of the steps: it is $E_2 E_1 A$, *not* $E_1 E_2 A$.  Because matrix multiplication is generally [not commutative](https://en.wikipedia.org/wiki/Commutative_property), $E_2 E_1$ and $E_1 E_2$ give *different* matrices.\n",
    "\n",
    "Notice, furthermore, that the matrices $E_1$ and $E_2$ are both **lower-triangular matrices**.  This is a consequence of the structure of Gaussian elimination (assuming no row re-ordering): we always add the pivot row to rows *below* it, never *above* it.\n",
    "\n",
    "The *product* of lower-triangular matrices is always lower-triangular too.  In consequence, the product $E = E_2 E_1$ is lower-triangular, and Gaussian elimination can be viewed as yielding\n",
    "\n",
    "$$EA=U$$\n",
    "\n",
    "where $E$ is lower triangular and $U$ is upper triangular!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse elimination: LU factors\n",
    "\n",
    "However, in practice, it turns out to be more useful to write this as\n",
    "\n",
    "$$A= E^{-1} U = E_1^{-1}E_2^{-1}U$$\n",
    "\n",
    "where $E^{-1}$ is the [inverse of the matrix](http://mathworld.wolfram.com/MatrixInverse.html) $E$.  It is the matrix that **reverses the steps** of Gaussian elimination, taking us back from $U$ to $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result, therefore, is that Gaussian elimination (without row swaps) can be viewed as a *factorization* of the original matrix $A$\n",
    "$$\n",
    "A = LU\n",
    "$$\n",
    "into a **product of lower- and upper-triangular matrices**.  (Furthermore, although we didn't comment on this above, $L$ is always 1 along its diagonal.)  This factorization is called the [LU factorization](https://en.wikipedia.org/wiki/LU_decomposition) of $A$.  When a computer performs Gaussian elimination, what it computes are the $L$ and $U$ factors.\n",
    "\n",
    "What this accomplishes is to break a complicated matrix $A$ into **much simpler pieces** $L$ and $U$.  It may not seem at first that $L$ and $U$ are *that* much simpler than $A$, but they are: lots of operations that are very difficult with $A$, like solving equations or computing the determinant, become *easy* once you known $L$ and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting $U$ was \"easy\", it is just Gaussian elimination. But to get $L$, we first write out the individual elimination steps as matrices, then invert them to move them to the other side, then multiply them together to get $L$.\n",
    "\n",
    "However, we can just \"read off\" $L$ much more simply directly from the pivot-row \"multipliers\" that we use during elimination steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We'll do Gaussian elimination on the following 3×3 invertible (non-singular) matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 2 & 5 & 1 \\\\ -3 & 1 & -1 \\end{pmatrix} \\stackrel{r_2 - \\color{red}{2}r_1}{\\stackrel{r_3 + \\color{red}{3}r_1}{\\longrightarrow}}\n",
    "    \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 0 & \\color{blue}{1} & 1 \\\\  0 & 7 & -1 \\end{pmatrix} \\stackrel{r_3 - \\color{red}{7}r_2}{\\longrightarrow}\n",
    "    \\begin{pmatrix} \\color{blue}{1} & 2 & 0 \\\\ 0 & \\color{blue}{1} & 1 \\\\  0 & 0 & \\color{blue}{-8} \\end{pmatrix} = U\n",
    "$$\n",
    "\n",
    "(Here, \"$r_2 - 2r_1$\" denotes the elimination step \"row 2 - 2(row 1)\" etcetera.)\n",
    "\n",
    "To get $L$, we just need to write down the multipliers as we go along, with **opposite signs**, putting each multiplier in the **same column and row** as the corresponding elimination step:\n",
    "\n",
    "$$\n",
    "L = \\begin{pmatrix} 1 & & \\\\ \\color{red}{+2} & 1 & \\\\ \\color{red}{-3} & \\color{red}{+7} & 1 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Using LU factorizations\n",
    "\n",
    "Lots of things that you might want to do with a matrix $A$ become easier once you have the $A=LU$ factorization.  Most importantly, it becomes much easier to solve systems of equations.\n",
    "\n",
    "(for an $n \\times n$ matrix $A$, it takes around $n^3$ operations to perform Gaussian elimination to get $U$ and $L$, but subsequently solving for $x$ by takes only around $n^2$ operations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ax=b\n",
    "1. Factor $A = LU$ by Gaussian elimination (not including row swaps, discussed below!), giving $Ax = b \\implies LUx = L(Ux) = b$\n",
    "2. Let $c = Ux$.  Solve $Lc = b$ for $c$ by forward-substitution.  (*Note:* this is especially easy because $L$ has only 1's on the diagonal, meaning that there are no divisions.)\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution.\n",
    "\n",
    "The key point to realize is that solving $Lc = b$ for $c$ involves *exactly the same elimination steps* as if you had augmented the matrix with $b$ during Gaussian elimination.   The bookkeeping is more tedious for a human, but computers are good at bookkeeping, and there turn out to be several practical advantages for computer software to separate solving for $LU$ and solving for $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's do another example, this one small enough to do by hand, using our 3×3 example from earlier.  Let's solve:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 0 \\\\ 2 & 5 & 1 \\\\ -3 & 1 & -1 \\end{pmatrix}}_{A = LU} \\underbrace{\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}}_x = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 15 \\\\ -4 \\end{pmatrix}}_b\n",
    "$$\n",
    "\n",
    "First we solve $Lc = b$ by forward substitution:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 &  &  \\\\ 2 & 1 &  \\\\ -3 & 7 & 1 \\end{pmatrix}}_{L} \\underbrace{\\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\end{pmatrix}}_c = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 15 \\\\ -4 \\end{pmatrix}}_b\n",
    "$$\n",
    "\n",
    "This immediately gives $c_1 = 5$ from the first row, $2c_1 + c_2 = 10 + c_2 = 15 \\implies c_2 = 5$ from the second row, and $-3c_1 + 7c_2 + c_3 = -15 + 35 + c_3 = -4 \\implies c_3 = -24$  from the third row.  If you look carefully, and remember the Gaussian-elimination steps that we did on $A$, you'll see that these are in fact exactly the same elimination steps applied to $c$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it matches our hand calculation!  Now we solve\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} 1 & 2 & 0 \\\\  & 1 & 1 \\\\  &  & -8 \\end{pmatrix}}_{U} \\underbrace{\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}}_x = \n",
    "\\underbrace{\\begin{pmatrix} 5 \\\\ 5 \\\\ -24 \\end{pmatrix}}_c\n",
    "$$\n",
    "\n",
    "by backsubstitution. The third row gives $-8 x_3 = -24 \\implies x_3 = 3$.  The second row gives $x_2 + x_3 = x_2 + 3 = 5 \\implies x_2 = 2$, and the first row gives $x_1 + 2x_2 = x_1 + 4 = 5 \\implies x_1 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row swaps and PA = LU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have mostly ignored the possibility of row swaps.  Row swaps may be *required* if you encounter a zero pivot (assuming there is a nonzero value below it in the same column), but this is often unlikely to occur in practice (especially for random matrices!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LU factorization with row swaps corresponds to a factorization\n",
    "\n",
    "$$\n",
    "PA = LU\n",
    "$$\n",
    "\n",
    "Now, to solve $Ax = b$, a more complete process is:\n",
    "\n",
    "1. Factor $PA = LU$\n",
    "2. Multiply $P$ by both sides to give $PAx = LUx = Pb$\n",
    "3. Let $c=Ux$ and solve $Lc = Pb$ for $c$ by forward-substitution\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular matrices\n",
    "\n",
    "If we encounter a zero pivot (or even just a small pivot, on a computer) during Gaussian elimination, we normally swap rows to bring a nonzero pivot up from a subsequent row.  However, what if there are *no* nonzero values below the pivot in that column?  This is called a [singular matrix](https://en.wikipedia.org/wiki/Invertible_matrix): we can still proceed with Gaussian elimination, but **we can't get rid of the zero pivot**.\n",
    "\n",
    "If you have $Ax=b$ where $A$ is singular, then there will typically (for most right-hand sides $b$) be **no solutions**, but there will occasionally (for very special $b$) be **infinitely many solutions**.  (For $2 \\times 2$ matrices, solving $Ax=b$ corresponds to finding the intersection of two lines, and a singular case corresponds to two parallel lines — either there are no intersections, or they intersect everywhere.)\n",
    "\n",
    "For example, consider the following $4 \\times 4$ matrix $A=LU$:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -1 & 1 &  8 \\\\\n",
    "             6 &  1 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  1 &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "The third pivot in $U$ is zero!   Now, suppose we want to solve $Ax=b$.  We first solve $Lc=b$ to apply the elimination steps to $b$.  This is no problem since $L$ has 1's along the diagonal. Suppose we get $c = (c_1, c_2, c_3, c_4)$.  Then we proceed by backsubstitution to solve $Ux = c$, starting with the last row of $U$:\n",
    "\n",
    "$$\n",
    "1 \\times x_4 = c_4 \\implies x_4 = c_4 \\\\\n",
    "\\color{red}{0 \\times x_3} - 2 \\times x_4 = c_3 \\implies \\mbox{no solution unless } -2 x_4 = -2 c_4 = c_3\n",
    "$$\n",
    "For very special right-hand sides, where $c_3 = 2c_4$, we can plug in *any* $x_3$ and get a solution (infinitely many solutions).  Otherwise, we get *no* solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that singular cases are not very interesting.  In reality, **exactly singular square matrices never occur by accident**.  There is always some *deep structure of the underlying problem* that causes the singularity, and understanding this structure is *always* interesting.\n",
    "\n",
    "On the other hand, **nearly singular** matrices (where the pivots are nonzero but very small) *can* occur by accident, and dealing with them is often a delicate problem because they are very sensitive to roundoff errors.  (We call these matrices [ill-conditioned](https://en.wikipedia.org/wiki/Condition_number).) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrices are **more singular than others**.  For example, they can have **two zero pivots**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  8 \\\\\n",
    "             6 &  3 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **three**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  2 \\\\\n",
    "             6 &  3 & 4 & -2 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **four**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             0 & -1 & 0 &  3 \\\\\n",
    "             0 & -2 & 1 &  2 \\\\\n",
    "             0 &  3 & 4 & -2 \\\\\n",
    "             0 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                \\color{red}{0} & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "(Notice how changing only one pivot changes only one column of $A$: each column of $U$ determines one column of $A$ via our \"matrix × columns\" viewpoint on matrix multiplication.)\n",
    "\n",
    "Intuitively, having more zero pivots seems \"more singular\", and requires \"more coincidences\" in the right-hand side to have a solution, and has a \"bigger infinity\" of solutions when there *is* a solution.   We will quantify all of these intuitions later in 18.06, when we begin discussing the [null space](https://en.wikipedia.org/wiki/Kernel_(linear_algebra) and [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra) of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Suppose that you are *given* the $PA = LU$ factorization of an invertible $m \\times m$ matrix $A$, and we want to solve the block-matrix equation:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} A & B \\\\ 0 & A \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\n",
    "= \\begin{pmatrix} b \\\\ c \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $B$ is an $m \\times m$ matrix, \"0\" denotes an $m \\times m$ block of zeros, and x,y,b,c are m-component vectors.\n",
    "\n",
    "Write the solution vectors x,y in terms of P,L,U,B,b,c (or the inverses of these matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
